* Hadoop简介
1. 比较SQL数据库和Hadoop
   1) 用向外扩展代替向上扩展，（分布式系统，俗称向外扩展；大型单机服务器，俗称向上扩展）
   2) 用键/值对代替关系表，关系型数据库一个基本原则是让数据按照某种模式存放在具有关系型数据结构的表中。在Hadoop中，数据的来源可以有任何形式，但最终会转化为键/值对以供处理。
   3) 用函数式（MapReduce）代替声明式查询（SQL）。SQL查询数据的手段是，声明想要查询结果并让数据库引擎判定如何获取数据，在MapReduce中，实际的数据处理步骤是由用户指定。SQL使用查询语句，而MapReduce则使用脚本和代码。
   4) 用离线批量处理代替在线处理。Hadoop是专为离线处理和大规模数据分析而设计的，并不适合对几个记录随机读写在线事务处理模式。
2. MapReduce是一个数据处理模型，它最大的优点是容易扩展到多个计算节点上处理数据，在MapReduce模型中，数据处理原语被称为mapper和reducer。
3. MapReduce程序的执行分为两个主要阶段，为mapping和reducing。每个阶段均定义为一个数据处理函数，分别被称为mapper和reducer。在mapping阶段，MapReduce获取输入数据并将数据单元装入mapper。在reduceing阶段，reducer处理来自mapper的所有输出，并给出最终结果。简而言之，mapper意味着将输入进行过滤和转换，使reducer可以完成聚合。
  |        | 输入           | 输出           |
  |--------+----------------+----------------|
  | map    | <k1, v1>       | list(<k2, v2>) |
  | reduce | <k2, list(v2)> | list(<k3, v3>) |
4. 在MapReduce框架中编写应用程序就是定制化mapper和reducer的过程。
   1) 应用的输入必须组织为一个键/值对的列表list(<k2, v2>)。用于处理多个文件的输入格式通常为list(<string filename, string file_content>)，用于处理日志文件这种大文件的输入格式为list(<integer line_number, string log_event>)。
   2) 含有键/值对的列表被拆分，进而通过调用mapper的map函数对每个单独的键/值对<k1, v1>进行处理。在这里，键k1经常被mapper所忽略。mapper转换每个<k1, v1>对并将之放入<k2, v2>对的列表种。处理键/值对可以采用任意的顺序，而且，这种转换必须时封闭的，使得输出仅依赖于一个单独的键/值对。
   3) 所有mapper的输出(在概念上)被聚合到一个包含<k2, v2>对的巨大列表中。所有共享相同k2的对被组织在一起形成一个新的键/值对<k2, list(v2)>。框架让reducer来分别处理每个被聚合起来的<k2, list(v2)>。

* 初识hadoop
1. 在一个全配置的集群上，“运行hadoop”意味着在网络分布的不同服务器上运行一组守护进程(daemons)。这些守护进程有特殊的角色，一些存在单个服务器上，一些则运行在多个服务器上。它们包括：
   1. namenode(名字节点)：hadoop在分布式计算和分布式存储中都采用了主/从(master/slave)结构。分布式存储系统被称为hadoop文件系统，或者简称hdfs。namenode位于hdfs的主端，它指导从端的datanode执行底层的i/o任务。namenode跟踪文件如何被分割成文件块，而这些块又被哪些节点存储，以及分布式文件系统的整体运行状态是否正常。运行namenode会消耗大量的内存和i/o资源。
   2. datanode(数据节点):每一个集群上的从节点都会驻留一个datanode守护进程，来执行分布式文件系统的繁重工作---将hdfs数据块读取或者写入到本地文件系统的实际文件中。
   3. secondary namenode(次名字节点)：用于监测hdfs集群状态的辅助守护进程。像namenode一样，每个集群有一个snn，它同常独占一台服务器，该服务器不会运行其他的datanode或者tasktracer守护进程。snn与namenode的不同在于它不接收或记录hdfs的任何实时变化。相反它与namenode通信，根据集群所配置的时间间隔获取hdfs元数据的快照。
   4. jobtracker(作业跟踪节点)：jobtracker守护进程时应用程序和hadoop之间的纽带。一旦提交代码到集群上，jobtracker就会确定执行计划。通常运行在服务器集群的主节点上。
   5. tasktracker(任务跟踪节点):与存储的守护进程一样，计算的守护进程也遵循主/从架构：jobtracker作为主节点，检测mapreduce作业的整个执行过程，同时，tasktracker管理各个任务在每个从节点上的执行情况。每个tasktracker负责执行由jobtracker分配的单项任务。tasktracker持续不断地与jobtracker通信。

* hadoop组件
1. hdfs是一种文件系统，专为mapreduce这类框架下的大规模分布式数据处理而设计。
2. hadoop的文件命令采取的形式为 hadoop fs -cmd <args>，其中cmd是具体的文件命令，而<args>是一组数目可变的参数，cmd的命令通常与unix对应的命令相同。
3. hadoop文件api的起点时filesystem类。这是一个与文件系统交互的抽象类，存在不同的具体实现子类用来处理hdfs和本地文件系统，可以通过调用factory方法filesystem.get(configuration conf)来得到所需的filesystem实例。configuration类是用于保存键/值配置参数的特殊类。它的默认实例化方法是以hdfs系统的资源配置为基础的。
4. mapreduce程序通过操作键/值对来处理数据，一般形式为
   map:(k1, v1)->list(k2, v2)
   reduce:(k2, list(v2))->list(k3, v3)
5. 为了让键/值对可以在集群上移动，mapreduce框架提供了一种序列化键/值对的方法。实现writable接口的类可以是值，而实现writablecomparable<t>接口的类可以是键也可以是值。
6. 一个类要作为mapper，需继承mapreducebase基类并实现mapper接口，mapper和reducer的基类均为mapreducebase类。void configure(jobconf job)。该函数提供提取xml配置文件或者应用程序主类中的参数，在数据处理之前调用该函数。void close()。作为map任务结束前的最后一个操作，该函数完成所有的结尾工作。
7. Mapper接口负责数据处理阶段。采用Mapper<K1,V1,K2,V2>Java泛型。Mapper只有一个方法map，用于处理一个单独的键/值对。
8. reducer的实现和mapper一样必须首先在MapReduce基类上扩展，允许配置和清理。当reducer任务接收来自各个mapper的输出时，它按照键/值对中的键对输入数据进行排序，并将相同的值归并。然后调用reduce()函数，并通过迭代处理那些与指定键相关联的值。生成一个列表(K3,V3)。OutputCollector接收reduce阶段的输出，并写入输出文件。
9. Hadoop通过HashPartitioner类强制执行对键进行散列来确定reducer。
10. MapReduce将输入数据分割成块，这些块可以在多台计算机上并行处理，这些块被称为输入分片(Input Split)。FSDataInputStream扩展了DataInputStream以支持随机读。
11. 常用的InputFormat类
  | TextInputFormat               | key:LongWritable value:Text |
  |-------------------------------+-----------------------------|
  | KeyValueTextInputFormat       | key:Text value:Text         |
  |-------------------------------+-----------------------------|
  | SequenceFileInputFormat<K, V> | key:K value:V               |
  |-------------------------------+-----------------------------|
  | NLineInputFormat              | key:LongWritable value:Text |
12. InputFormat需要执行两个功能：1、getSplits()方法，确定所有用于输入数据的文件，并将之分割为输入分片，每个map任务分配一个分片。2、getRecordReader()提供一个对象(RecordReader),循环提取给定分片中的记录，并解析每个记录为预定义类型的键与值。
13. MapReduce输出数据到文件使用outputFormat类。输出文件放在一个公用目录中，通常命名为part-nnnn，nnnn是reducer的分区ID。RecordWriter对象将输出结果进行格式化，而RecordReader对输入格式进行解析。
 | TextOutputFormat<K, V>         | 将每个记录写成一行文本，键和值以字符串的形式写入，并以制表符分隔 |
 |--------------------------------+------------------------------------------------------------------|
 | SequenceFileOutputFormat<K, V> | 以Hadoop专有序列文件格式写入键/值对                              |
 |--------------------------------+------------------------------------------------------------------|
 | NullOutputFormat<K, V>         | 无输出                                                           |
14. 

 
